# **ML-Inventory-Activity**
![Version](https://img.shields.io/badge/Version-1.1.1-brightgreen.svg) ![Dashboard](https://img.shields.io/badge/Dashboard-TODO-red.svg) ![Documentation](https://img.shields.io/badge/Documentation-Ready-brightgreen.svg)

> Deploys a Jenkins Pipeline which automates the daily aggregation and transformation of data and insight relating to inventory of active listings on the cars.com platform via automic. Built in a python environment using Spark via the pySpark API, EC2 spot instances are leveraged to pull from various AWS data storage solutions.
---

## Table of contents
1. config
    - There are several config files, for dev, prod, stage, and forge. 
        - The forge file is used for spinning up an EC2 instance using forge.
        - The other config files are used for identifying the various filepaths in the data-lake from which the data is being pulled from.
2. resources
    - The resource files have two main functions, one is defining the Automic environment for the workflow and serving as config files for the python scripts by providing locations for various required datasets. There are also json files which define the schema for the final IA df, and the ingestion method for writing to s3. 
3. scripts
    - The shell scripts found here are used for initiating the pipeline, creating EC2 clusters and their environments and starting the python scripts. They also do error handling, basic logging functions, and hold the looping logic for completing backfill jobs.
4. src
    - This source folder contains all of the pySpark scripts used by this pipeline. Shared definitions are stored within the functions.py file.
        - inventory_activity_main holds the main function which executes the ETL process in a linear path.
        - active_listing_tmp extracts current data from vehicle_temp and filters it against data from vehicle_history_temp to create a subset of unique active inventory records in a dataframe object.
        - listing_activity pulls weighted impressions and leads to build an accurate activity tracking metric per listing.
        - active_listing_transform_tmp pulls vehicle data on active listings from various datasets in s3 and joins them to the IA df. 
        - listing_activity_load is used to compile the final attributes of the vehicle/listing from various sources including competitor data. The data is written to a landing bucket in s3, from which it is moved into the core bucket. 
5. miscellaneous
    - Jenkinsfile is a groovy script used to build the Pipeline and link it to necessary dependencies. 

## Process

##### Flow

The python scripts are executed on a linear path, each having some contribution to the final IA table. This flow is initiated with inventory_activity_main.py. 
1. inventory_activity_main.py
    - This script is responsible for executing the remaining py scripts, and it holds all of the intermediate products generated by and for the other scripts. The main spark session is also initiated here. 
2. active_listing_tmp.py
    - Taking the spark session, config data, and the date as its parameters the main function here has the job of extracting all unique active vehicle listings which are derived from the vehicle_temp and vehicle_history_temp tables. Both tables are filtered within a date range as to specify "current/active" and a union between the two returns the unique listings. 
3. listing_activity_tmp.py
    - With inputs of the spark session, config data, the date, and the df generated in the previous script, the main method here has the task of consolidating all leads and impressions relevant to all active listings. The leads & impressions are collected on rolling intervals of 2, 3, 5, 7, and 30 days.
4. active_listing_transform_tmp.py
    - The main function here pulls directly from s3 {make, make_model, model_year, vehicle_trim, stock_type, bodystyle, zip_code, dma_zip, dealer_review_rollup, dealer_review, vehicle_definition} to make several temp dfs. The dealer_review df is filtered by a "purchased" flag and the dma_zip df is joined to the zip_code. These two new dfs along with the previously created temp dfs are all joined to return a veh_df which is returned back to the main script.
5. inventory_activity_load.py
    - This is the final part of the IA workflow. First a few temp tables are registered from s3 {vehicle_daily_temp, vehicle_review_rollup_temp, customer_temp, activity_customer_temp, dealer_sales_locations}, which are joined onto the IA df. Listing values from carguru, truecar, and autotrader are filtered by matches to listings on cars.com which are then joined to the IA df as well. To match with the registered schema on s3, the data types of some columns are casted accordingly. Finally the df is broken into several random partitions which are then written to s3 in a landing bucket, ready to be sent to the core bucket. 


---

## Business Use

This workflow generates a lot of relevant information on active listings that are used for reporting purposes by the business intelligence team and a lot of the data is also consumed by models created by the data science team. This job is highly essential as it has a lot of dependent process on the analytics side and also impacts the frontend of the cars.com platform from the data it gathers. 


## Dependent Jobs

- dealer_activity
- best_match
- Listing_quality_score
- listing_quality_velocity
- market_supply
- ml_hotcar
- ml_lead_dollar_value_all
- similar_vehicle
- price_badging
- prospect_entitiy
- tiv_daily / weekly
- market_valuation
- cdp_batch_feature

#### This repository was built by and for the MLaaS team at Cars.com. 